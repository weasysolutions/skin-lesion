{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *Lesiones de la piel*\n",
    "\n",
    "\n",
    "### Aprendizaje profundo: Redes Neuronales Convolutivas con TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivación\n",
    "\n",
    "La dermatoscopia es una técnica de diagnóstico ampliamente utilizada que mejora el diagnóstico de lesiones cutáneas benignas y malignas en comparación con el examen a simple vista. Las imágenes dermatoscópicas también son una fuente adecuada para entrenar redes neuronales artificiales para diagnosticar automáticamente lesiones cutáneas pigmentadas. \n",
    "\n",
    "Los avances recientes en las capacidades de las tarjetas gráficas y las técnicas de aprendizaje en máquinas establecen nuevos puntos de referencia con respecto a la complejidad de las redes neuronales y aumentan las expectativas de que pronto estarán disponibles sistemas de diagnóstico automatizados que diagnostican todo tipo de lesiones cutáneas pigmentadas sin la necesidad de experiencia humana (https://arxiv.org/abs/1803.10417).\n",
    "\n",
    "La capacitación de algoritmos de diagnóstico basados ​​en redes neuronales requiere un gran número de imágenes anotadas (o etiquetadas), pero el número de imágenes dermatoscópicas de alta calidad con diagnósticos confiables está limitado o restringido a solo unas pocas clases de enfermedades.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### El archivo ISIC (International Skin Imaging Collaboration)\n",
    "\n",
    "El archivo ISIC (https://isic-archive.com/) es una colección de múltiples bases de datos e incluye actualmente 13786 imágenes dermatoscópicas (a partír del 12 de febrero de 2018). Es la fuente estándar para la investigación de análisis de imágenes dermatoscópicas. Sin embargo, está sesgada hacia las lesiones melanocíticas (12893 de 13786 imágenes son nevi ormelanomas). \n",
    "\n",
    "Debido a que este portal es el recurso más completo, técnicamente avanzado y accesible para la dermatoscopia digital, proporcionaremos nuestro conjunto de datos a través del archivo ISIC. Debido a las limitaciones de los conjuntos de datos disponibles, las investigaciones anteriores se centraron en las lesiones melanocíticas (es decir, la diferenciación entre melanoma y nevus) y No se tienen en cuenta las lesiones pigmentadas no melanocíticas, aunque son comunes en la práctica. \n",
    "\n",
    "Para impulsar la investigación sobre el diagnóstico automatizado de imágenes dermatoscópicas, recientemente investigadores de la Univ. Médica de Vienna y la Universidad de Queensland lanzaron el conjunto de datos HAM10000 [“Human Against Machine con 10000 imágenes de entrenamiento”]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "*El artículo se encuentra en la siguiente liga: <br>*\n",
    "> [El conjunto de datos HAM10000: una gran colección de imágenes dermatoscópicas de múltiples fuentes de lesiones cutáneas pigmentadas comunes](https://arxiv.org/abs/1803.10417)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6f77793e7ebbcd523899bafaa9dae9763b9a0a5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Introduction y Objetivo de la Lección **\n",
    "\n",
    "Este libro detalla un proceso que construye el modelo y luego convertirlo de Keras a Tensorflow.js. El código javascript, html y css para la aplicación está disponible en github. <br>\n",
    "\n",
    "Si un modelo tiene una precisión del 60%, por lo general se consideraría un modelo malo. Sin embargo, si también tiene una precisión del 3% superior al 90% y el objetivo requiere que produzca 3 predicciones, entonces puede ser un buen modelo.\n",
    "\n",
    "*Este es el objetivo que se ha definido para esta tarea:*\n",
    "\n",
    "> Crear una herramienta en línea que pueda decirle a los médicos y tecnólogos de laboratorio los tres diagnósticos de mayor probabilidad para una lesión cutánea determinada. Esto les ayudará a identificar rápidamente a los pacientes de alta prioridad y acelerar su flujo de trabajo. La aplicación debe producir un resultado en menos de 3 segundos. Para garantizar la privacidad, las imágenes se deben preprocesar y analizar localmente y nunca se deben cargar en un servidor externo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [ Descripción de las categorías del diagnóstico:](https://arxiv.org/abs/1803.10417) <br>\n",
    "\n",
    " **nv** <br>\n",
    " Los nevos melanocíticos son neoplasias benignas de melanocitos y aparecen en una gran variedad de variantes, que están incluidas en nuestra serie. Las variantes pueden diferir significativamente desde un punto de vista dermatoscópico. <br>\n",
    " *[6705 imágenes]*\n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    " **mel** <br>\n",
    " El melanoma es una neoplasia maligna derivada de melanocitos que puede aparecer en diferentes variantes. Si se extirpa en una etapa temprana, se puede curar por escisión quirúrgica simple. Los melanomas pueden ser invasivos o no invasivos (in situ). Se incluyen todas las variantes de melanoma, incluido el melanoma in situ, pero se excluye el melanoma no pigmentado, subungueal, ocular o mucoso. <br> \n",
    " *[1113 imágenes]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [ Descripción de las categorías del diagnóstico:](https://arxiv.org/abs/1803.10417) <br>\n",
    " \n",
    "** bkl ** <br>\n",
    " La \"queratosis benigna\" es una clase genérica que incluye queratosis seborreica (\"verruga senil\"), lentigo solar, que puede considerarse una variante plana de la queratosis seborreica y liquenoplus como queratosis (LPLK), que corresponde a una seborréica Queratosis o un lentigo solar con inflamación y regresión [22]. Los tres subgrupos pueden tener un aspecto dermatoscópico diferente, pero los agrupamos porque son biológicamente similares y, a menudo, se informan bajo el mismo término genérico histopatológicamente. Desde un punto de vista dermatoscópico, las queratosis de tipo liquen plano son especialmente desafiantes porque pueden mostrar características morfológicas que simulan un melanoma [23] y con frecuencia se realizan biopsias o se extirpan por razones diagnósticas.\n",
    "*[1099 imágenes]*\n",
    "***\n",
    "**bcc** <br>\n",
    "El carcinoma de células basales es una variante común del cáncer de piel epitelial que rara vez hace metástasis pero crece destructivamente si no se trata. Aparece en diferentes variantes morfológicas (plana, nodular, pigmentada, quística, etc.), todas incluidas en este conjunto. <br>\n",
    "*[514 imágenes] *\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3f6843b78793e1c047ca6909a7449dc9bfc3f1c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "** akiec ** <br>\n",
    "Las queratosis actínicas (queratosis solares) y el carcinoma intraepitelial (enfermedad de Bowen) son variantes no invasivas comunes del carcinoma de células escamosas que pueden tratarse localmente sin cirugía. Algunos autores los consideran como precursores de carcinomas de células escamosas y no como carcinomas reales. Sin embargo, hay acuerdo en que estas lesiones pueden progresar a un carcinoma de células escamosas invasivo, que generalmente no está pigmentado. Ambas neoplasias comúnmente muestran escamas en la superficie y comúnmente están desprovistas de pigmento. Las queratosis actínicas son más comunes en la cara y la enfermedad de Bowen es más común en otros sitios del cuerpo. Debido a que ambos tipos están inducidos por la luz ultravioleta, la piel circundante generalmente se caracteriza por daños severos causados ​​por el sol, excepto en los casos de enfermedad de Bowen que son causados por la infección del virus del papiloma humano y no por los rayos UV. Existen variantes pigmentadas para la enfermedad de Bowen [19] y para las queratosis actínicas [20]. Ambos están incluidos en este conjunto. <br>\n",
    "*[327 imágenes]*\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** vasc ** <br>\n",
    "Las lesiones vasculares de la piel en el conjunto de datos van desde los angiomas de cereza hasta los angioqueratomas [25] y los granulomas piógenos [26]. La hemorragia también se incluye en esta categoría. <br>\n",
    "*[142 imágenes]*\n",
    "\n",
    "** df ** <br>\n",
    "El dermatofibroma es una lesión benigna de la piel considerada como una proliferación benigna o una reacción inflamatoria a un trauma mínimo. Es marrón y con frecuencia muestra una zona central de fibrosis dermatoscópica [24]. <br>\n",
    "*[115 imágenes]*\n",
    "\n",
    "\n",
    "<br> * [Imágenes totales = 10015] *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [TensorFlow](https://www.tensorflow.org/)\n",
    "\n",
    "TensorFlow es una plataforma de código abierto  para el aprendizaje automático. Cuenta con un ecosistema integral y flexible de herramientas, bibliotecas y recursos de la comunidad que permite a los investigadores impulsar el estado de la técnica en ML y los desarrolladores pueden crear y desplegar fácilmente aplicaciones potenciadas por ML.\n",
    "\n",
    "### [Keras](https://keras.io/)\n",
    "\n",
    "Keras es una interfaz de programación de aplicaciones (API) de redes neuronales de alto nivel, escrita en Python y capaz de ejecutarse sobre TensorFlow, CNTK o Theano. Fue desarrollado con un enfoque en permitir la experimentación rápida. Poder pasar de la idea al resultado con el menor retraso posible es clave para hacer una buena investigación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Aprendizaje Profundo \n",
    "\n",
    "La mayoría de los modelos modernos de aprendizaje profundo se basan en una red neuronal artificial,\n",
    "\n",
    "En el aprendizaje profundo, cada nivel aprende a transformar sus datos de entrada en una representación un poco más abstracta y compuesta. En una aplicación de reconocimiento de imágenes, la entrada sin formato puede ser una matriz de píxeles; la primera capa de representación puede abstraer los píxeles y codificar los bordes; la segunda capa puede componer y codificar arreglos de bordes; la tercera capa puede codificar una nariz y ojos; y la cuarta capa puede reconocer que la imagen contiene una cara. Es importante destacar que un proceso de aprendizaje profundo puede aprender qué características ubicar de manera óptima en qué nivel por sí solo. (Por supuesto, esto no elimina completamente la necesidad del ajuste manual; por ejemplo, un número variable de capas y tamaños de capas puede proporcionar diferentes grados de abstracción). \n",
    "\n",
    "El \"profundo\" en \"aprendizaje profundo\" se refiere al número de capas a través de las cuales se transforman los datos. Más precisamente, los sistemas de aprendizaje profundo tienen una profundidad de ruta de asignación de crédito (CAP) sustancial. El CAP es la cadena de transformaciones de entrada a salida. Los CAP describen conexiones potencialmente causales entre entrada y salida. Para una red neuronal avanzada, la profundidad de los CAP es la de la red y es el número de capas ocultas más una (ya que la capa de salida también está parametrizada). Para redes neuronales recurrentes, en las que una señal puede propagarse a través de una capa más de una vez, la profundidad de la PAC es potencialmente ilimitada. [2] No se acordó universalmente que el umbral de profundidad divide el aprendizaje superficial del aprendizaje profundo, pero la mayoría de los investigadores están de acuerdo en que el aprendizaje profundo implica una profundidad de la PAC. cita requerida] Más allá de que más capas no se agregan a la función de aproximación de la función de la red. Los modelos profundos (CAP> 2) son capaces de extraer mejores características que los modelos poco profundos y, por lo tanto, las capas adicionales ayudan en las características de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Aprendizaje Profundo y Redes Neuronales](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "#### Introducción.\n",
    "\n",
    "El sistema visual humano es una de las maravillas del mundo. Considere la siguiente secuencia de dígitos escritos a mano: \n",
    "<center> \n",
    "<a href=\"https://fontmeme.com/handwriting-fonts/\">\n",
    "<img src=\"https://fontmeme.com/permalink/190307/05e4494c338d7e33420364ec6abe0b42.png\" alt=\"handwriting-fonts\" border=\"0\"></a> \n",
    "</center>\n",
    "\n",
    "La mayoría de las personas reconocen sin esfuerzo esos dígitos como 570139. Esta facilidad engañosa. En cada hemisferio de nuestro cerebro, los humanos tienen una corteza visual primaria, también conocida como V1, que contiene 140 millones de neuronas, con decenas de miles de millones de conexiones entre ellas. Y, sin embargo, la visión humana implica no solo V1, sino toda una serie corticales visuales (V2, V3, V4 y V5) que realizan un procesamiento de imágenes cada vez más complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Aprendizaje Profundo y Redes Neuronales](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "La dificultad del reconocimiento de patrones visuales se hace evidente si intenta escribir un programa de computadora para reconocer dígitos como los anteriores. Lo que parece fácil cuando lo hacemos nosotros mismos de repente se vuelve extremadamente difícil. Las intuiciones simples sobre cómo reconocemos las formas, no son tan simples de expresar algorítmicamente. Cuando tratas de hacer precisas tales reglas, rápidamente te pierdes en una gran cantidad de excepciones y advertencias y casos especiales. Resulta desolador\n",
    "\n",
    "Las redes neuronales abordan el problema de una manera diferente. La idea es tomar un gran número de dígitos escritos a mano, conocidos como ejemplos de capacitación,\n",
    "\n",
    "<center> \n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png\"></a>  \n",
    "</center>\n",
    "\n",
    "y luego desarrollar un sistema que pueda aprender de esos ejemplos de entrenamiento. En otras palabras, la red neuronal utiliza los ejemplos para inferir automáticamente reglas para reconocer dígitos escritos a mano. Además, al aumentar el número de ejemplos de capacitación, la red puede aprender más sobre la escritura a mano, y así mejorar su precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *La mayoría de los modelos modernos de aprendizaje profundo se basan en una red neuronal artificial*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Aprendizaje Profundo y Redes Neuronales](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "#### Neuronas Artificiales: El Perceptrón\n",
    "\n",
    "Las redes de neuronales estan hechas de neuronas y existen varios tipos. Una de ellas es el perceptrón:\n",
    "\n",
    "\n",
    "<center> \n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz0.png\"></a>  \n",
    "</center>\n",
    "\n",
    "*Un perceptrón toma varias entradas binarias (tres en este caso), $x_{1}, x_{2}, x_{2}$ y produce una salida binaria única*. Una manera de pensar sobre el perceptrón es que es un dispositivo que toma decisiones al sopesar la evidencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [El Perceptrón](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "No es  muy realista, pero es fácil de entender. Suponga que se acerca el fin de semana, y has oído que habrá un festival de la cerveza en tu ciudad. Te gusta el queso y estás tratando de decidir si ir o no al festival. Puede tomar su decisión sopesando tres factores:\n",
    "\n",
    "¿El clima esta agradable?\n",
    "¿Tu novio o novia quiere acompañarte?\n",
    "¿Está el festival cerca del transporte público? (No tienes auto).\n",
    "\n",
    "Podemos representar estos tres factores mediante las variables binarias correspondientes $x_{1}, x_{2}, x_{2}$. Por ejemplo, tendríamos $x_{1} = 1$ si el clima es bueno, y $x_{1} = 0$ si el clima es malo. De manera similar, $x_{2} = 1$ si su novio o novia quiere ir, y $x_{2} = 0$ si no. Similarmente de nuevo para $x_{3} = 1$ y transporte público.\n",
    "\n",
    "Ahora, supongamos que adoras la cerveza, tanto que estás feliz de ir al festival, incluso si a tu novio o novia no le interesa y es difícil llegar al festival. Pero tal vez realmente detestas el mal tiempo, y no hay manera de que vayas al festival si el clima es malo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [El Perceptrón](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "Puede utilizar perceptrones para modelar este tipo de toma de decisiones. En este caso el perceptrón viene dado por \n",
    "![](../images/perceptron.png)\n",
    "\n",
    "Los números(pesos) 6,2,2 se han escogido de manera que reflejen el hecho que el clima es muy importante para usted, mucho más que si su novio o novia se unen a usted o la cercanía del transporte público. Finalmente, suponga que elige un umbral de 5 para el perceptrón. Con estas opciones, el perceptrón implementa el modelo de toma de decisiones deseado, generando 1 cuando el clima es bueno y 0 cuando el clima es malo. No importa el resultado si su novio o novia quiere ir, o si el transporte público está cerca. \n",
    "\n",
    "Al variar los pesos y el umbral, podemos obtener diferentes modelos de toma de decisiones. Por ejemplo, supongamos que, en cambio, elegimos un umbral de 3. El perceptrón decidirá que deberías ir al festival cuando el tiempo fuera bueno o cuando el festival estaba cerca del transporte público y tu novio o novia estaba dispuesto a unírsele. En otras palabras, sería un modelo diferente de toma de decisiones. Bajar el umbral significa que estás más dispuesto a ir al festival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [El Perceptrón](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "¡Obviamente, el perceptrón no es un modelo completo de toma de decisiones humanas! Pero lo que ilustra el ejemplo es cómo un perceptrón puede sopesar diferentes tipos de evidencia para tomar decisiones. Y debería parecer plausible que una compleja red de perceptrones pueda tomar decisiones bastante sutiles:\n",
    "<center> \n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz1.png\"></a>  \n",
    "</center>\n",
    "\n",
    "En esta red, la primera columna de perceptrones (lo que llamaremos la primera *capa* de perceptrones) está tomando tres decisiones muy simples, sopesando la evidencia de entrada. ¿Qué pasa con los perceptrones en la segunda capa? Cada uno de esos perceptrones está tomando una decisión sopesando los resultados del primer nivel de toma de decisiones. De esta manera, los  perceptrones en la sucesivos están tomando decisiones a un nivel más complejo y más abstracto que los perceptrones de sus capas anteriores:  una red de múltiples capas de perceptrones puede participar en la toma de decisiones sofisticada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [El Perceptrón](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "<center> \n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz1.png\" withd></a>  \n",
    "</center>\n",
    "\n",
    "En la red de arriba, los perceptrones parecen tener múltiples salidas. De hecho, todavía son de salida única. Las flechas de salida múltiple son simplemente una forma útil de indicar que la salida de un perceptrón se está utilizando como entrada para varios otros perceptrones. Es menos difícil de manejar que dibujar una sola línea de salida que luego se divide.\n",
    "\n",
    "_Resulta ser que se pueden idear __algoritmos de aprendizaje__ que pueden ajustar automáticamente los pesos de una red de neuronas artificiales. Esta sintonización ocurre en respuesta a estímulos externos, sin la intervención directa de un programador._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bSe han descrito a  los perceptrones como un método para sopesar la evidencia para tomar decisiones. Otra forma en que se pueden usar los perceptrones es calcular las funciones lógicas elementales que usualmente consideramos computación subyacente, funciones como __AND__, __OR__ y __NAND__. Por ejemplo, supongamos que tenemos un perceptrón con dos entradas, cada una con un peso de -2, y un sesgo general de 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n"
     ]
    }
   ],
   "source": [
    "print('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "371731306c3e504b191979706e826c247def88dc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import keras\n",
    "#from keras import backend as K\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "371731306c3e504b191979706e826c247def88dc"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import keras\n",
    "#from keras import backend as K\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "371731306c3e504b191979706e826c247def88dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrique/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "#Semilla para el generador de números aleatorios de numpy\n",
    "seed(101)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "#Semilla para el generador de números aleatorios de tensorflow\n",
    "set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import keras\n",
    "#from keras import backend as K\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "d5a0a200bfc57c5489eaa930255d9420a7d01c47"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ba4787e7b13e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input'"
     ]
    }
   ],
   "source": [
    "os.listdir('../input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "086162161ba405b800863e7d545b5917e5205984"
   },
   "source": [
    "### Create the directory structure\n",
    "\n",
    "In these folders we will store the images that will later be fed to the Keras generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d24ef21f9f2359b8bf6b3e7a0b8ab5a43daaf566"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a new directory\n",
    "base_dir = 'base_dir'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "\n",
    "#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n",
    "\n",
    "# now we create 7 folders inside 'base_dir':\n",
    "\n",
    "# train_dir\n",
    "    # nv\n",
    "    # mel\n",
    "    # bkl\n",
    "    # bcc\n",
    "    # akiec\n",
    "    # vasc\n",
    "    # df\n",
    " \n",
    "# val_dir\n",
    "    # nv\n",
    "    # mel\n",
    "    # bkl\n",
    "    # bcc\n",
    "    # akiec\n",
    "    # vasc\n",
    "    # df\n",
    "\n",
    "# create a path to 'base_dir' to which we will join the names of the new folders\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "\n",
    "# [CREATE FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n",
    "# Inside each folder we create seperate folders for each class\n",
    "\n",
    "# create new folders inside train_dir\n",
    "nv = os.path.join(train_dir, 'nv')\n",
    "os.mkdir(nv)\n",
    "mel = os.path.join(train_dir, 'mel')\n",
    "os.mkdir(mel)\n",
    "bkl = os.path.join(train_dir, 'bkl')\n",
    "os.mkdir(bkl)\n",
    "bcc = os.path.join(train_dir, 'bcc')\n",
    "os.mkdir(bcc)\n",
    "akiec = os.path.join(train_dir, 'akiec')\n",
    "os.mkdir(akiec)\n",
    "vasc = os.path.join(train_dir, 'vasc')\n",
    "os.mkdir(vasc)\n",
    "df = os.path.join(train_dir, 'df')\n",
    "os.mkdir(df)\n",
    "\n",
    "\n",
    "\n",
    "# create new folders inside val_dir\n",
    "nv = os.path.join(val_dir, 'nv')\n",
    "os.mkdir(nv)\n",
    "mel = os.path.join(val_dir, 'mel')\n",
    "os.mkdir(mel)\n",
    "bkl = os.path.join(val_dir, 'bkl')\n",
    "os.mkdir(bkl)\n",
    "bcc = os.path.join(val_dir, 'bcc')\n",
    "os.mkdir(bcc)\n",
    "akiec = os.path.join(val_dir, 'akiec')\n",
    "os.mkdir(akiec)\n",
    "vasc = os.path.join(val_dir, 'vasc')\n",
    "os.mkdir(vasc)\n",
    "df = os.path.join(val_dir, 'df')\n",
    "os.mkdir(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ae8d37fdee293aaffa71a79019dd7277f8288fc"
   },
   "source": [
    "### Create Train and Val Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "268503398ef61904e05a2c0b0667d589f08a19a8"
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('../input/HAM10000_metadata.csv')\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c46ea5967e095d31dcf144b6f57f0343878fa432"
   },
   "source": [
    "### Create a stratified val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53e4b7b152ed831a7d7516156ac300c0e6985ffc"
   },
   "outputs": [],
   "source": [
    "# this will tell us how many images are associated with each lesion_id\n",
    "df = df_data.groupby('lesion_id').count()\n",
    "\n",
    "# now we filter out lesion_id's that have only one image associated with it\n",
    "df = df[df['image_id'] == 1]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24720fe3ea9f2f4b571abd09ecfbb931d6429852"
   },
   "outputs": [],
   "source": [
    "# here we identify lesion_id's that have duplicate images and those that have only\n",
    "# one image.\n",
    "\n",
    "def identify_duplicates(x):\n",
    "    \n",
    "    unique_list = list(df['lesion_id'])\n",
    "    \n",
    "    if x in unique_list:\n",
    "        return 'no_duplicates'\n",
    "    else:\n",
    "        return 'has_duplicates'\n",
    "    \n",
    "# create a new colum that is a copy of the lesion_id column\n",
    "df_data['duplicates'] = df_data['lesion_id']\n",
    "# apply the function to this new column\n",
    "df_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08b7eef3e0ac4112f63b8fb26ce19d55483cbc04"
   },
   "outputs": [],
   "source": [
    "df_data['duplicates'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "995445dfda2745165a53e61f42615104b951d4af"
   },
   "outputs": [],
   "source": [
    "# now we filter out images that don't have duplicates\n",
    "df = df_data[df_data['duplicates'] == 'no_duplicates']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "39fde25b59a9452cf700c5b2ff82cc7cc45c4a33"
   },
   "outputs": [],
   "source": [
    "# now we create a val set using df because we are sure that none of these images\n",
    "# have augmented duplicates in the train set\n",
    "y = df['dx']\n",
    "\n",
    "_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n",
    "\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1df37227f7ce993d054ed5b8480ee724696fc210"
   },
   "outputs": [],
   "source": [
    "df_val['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08c5e12fcef2da5f49267a6b82161b2c52c2b20a"
   },
   "source": [
    "### Create a train set that excludes images that are in the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03715a6cf5aeb6430ee144a84eb10dde216c0fb9"
   },
   "outputs": [],
   "source": [
    "# This set will be df_data excluding all rows that are in the val set\n",
    "\n",
    "# This function identifies if an image is part of the train\n",
    "# or val set.\n",
    "def identify_val_rows(x):\n",
    "    # create a list of all the lesion_id's in the val set\n",
    "    val_list = list(df_val['image_id'])\n",
    "    \n",
    "    if str(x) in val_list:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'train'\n",
    "\n",
    "# identify train and val rows\n",
    "\n",
    "# create a new colum that is a copy of the image_id column\n",
    "df_data['train_or_val'] = df_data['image_id']\n",
    "# apply the function to this new column\n",
    "df_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n",
    "   \n",
    "# filter out train rows\n",
    "df_train = df_data[df_data['train_or_val'] == 'train']\n",
    "\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
