{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *Lesiones de la piel*\n",
    "\n",
    "\n",
    "### Aprendizaje profundo: Redes Neuronales Convolutivas con TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivación\n",
    "\n",
    "La dermatoscopia es una técnica de diagnóstico ampliamente utilizada que mejora el diagnóstico de lesiones cutáneas benignas y malignas en comparación con el examen a simple vista. Las imágenes dermatoscópicas también son una fuente adecuada para entrenar redes neuronales artificiales para diagnosticar automáticamente lesiones cutáneas pigmentadas. \n",
    "\n",
    "Los avances recientes en las capacidades de las tarjetas gráficas y las técnicas de aprendizaje en máquinas establecen nuevos puntos de referencia con respecto a la complejidad de las redes neuronales y aumentan las expectativas de que pronto estarán disponibles sistemas de diagnóstico automatizados que diagnostican todo tipo de lesiones cutáneas pigmentadas sin la necesidad de experiencia humana (https://arxiv.org/abs/1803.10417).\n",
    "\n",
    "La capacitación de algoritmos de diagnóstico basados ​​en redes neuronales requiere un gran número de imágenes anotadas (o etiquetadas), pero el número de imágenes dermatoscópicas de alta calidad con diagnósticos confiables está limitado o restringido a solo unas pocas clases de enfermedades.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### El archivo ISIC (International Skin Imaging Collaboration)\n",
    "\n",
    "El archivo ISIC (https://isic-archive.com/) es una colección de múltiples bases de datos e incluye actualmente 13786 imágenes dermatoscópicas (a partír del 12 de febrero de 2018). Es la fuente estándar para la investigación de análisis de imágenes dermatoscópicas. Sin embargo, está sesgada hacia las lesiones melanocíticas (12893 de 13786 imágenes son nevi ormelanomas). \n",
    "\n",
    "Debido a que este portal es el recurso más completo, técnicamente avanzado y accesible para la dermatoscopia digital, proporcionaremos nuestro conjunto de datos a través del archivo ISIC. Debido a las limitaciones de los conjuntos de datos disponibles, las investigaciones anteriores se centraron en las lesiones melanocíticas (es decir, la diferenciación entre melanoma y nevus) y No se tienen en cuenta las lesiones pigmentadas no melanocíticas, aunque son comunes en la práctica. \n",
    "\n",
    "Para impulsar la investigación sobre el diagnóstico automatizado de imágenes dermatoscópicas, recientemente investigadores de la Univ. Médica de Vienna y la Universidad de Queensland lanzaron el conjunto de datos HAM10000 [“Human Against Machine con 10000 imágenes de entrenamiento”]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "*El artículo se encuentra en la siguiente liga: <br>*\n",
    "> [El conjunto de datos HAM10000: una gran colección de imágenes dermatoscópicas de múltiples fuentes de lesiones cutáneas pigmentadas comunes](https://arxiv.org/abs/1803.10417)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6f77793e7ebbcd523899bafaa9dae9763b9a0a5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Introduction y Objetivo de la Lección **\n",
    "\n",
    "Este libro detalla un proceso que construye el modelo y luego convertirlo de Keras a Tensorflow.js. El código javascript, html y css para la aplicación está disponible en github. <br>\n",
    "\n",
    "Si un modelo tiene una precisión del 60%, por lo general se consideraría un modelo malo. Sin embargo, si también tiene una precisión del 3% superior al 90% y el objetivo requiere que produzca 3 predicciones, entonces puede ser un buen modelo.\n",
    "\n",
    "*Este es el objetivo que se ha definido para esta tarea:*\n",
    "\n",
    "> Crear una herramienta en línea que pueda decirle a los médicos y tecnólogos de laboratorio los tres diagnósticos de mayor probabilidad para una lesión cutánea determinada. Esto les ayudará a identificar rápidamente a los pacientes de alta prioridad y acelerar su flujo de trabajo. La aplicación debe producir un resultado en menos de 3 segundos. Para garantizar la privacidad, las imágenes se deben preprocesar y analizar localmente y nunca se deben cargar en un servidor externo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [ Descripción de las categorías del diagnóstico:](https://arxiv.org/abs/1803.10417) <br>\n",
    "\n",
    " **nv** <br>\n",
    " Los nevos melanocíticos son neoplasias benignas de melanocitos y aparecen en una gran variedad de variantes, que están incluidas en nuestra serie. Las variantes pueden diferir significativamente desde un punto de vista dermatoscópico. <br>\n",
    " *[6705 imágenes]*\n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    " **mel** <br>\n",
    " El melanoma es una neoplasia maligna derivada de melanocitos que puede aparecer en diferentes variantes. Si se extirpa en una etapa temprana, se puede curar por escisión quirúrgica simple. Los melanomas pueden ser invasivos o no invasivos (in situ). Se incluyen todas las variantes de melanoma, incluido el melanoma in situ, pero se excluye el melanoma no pigmentado, subungueal, ocular o mucoso. <br> \n",
    " *[1113 imágenes]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [ Descripción de las categorías del diagnóstico:](https://arxiv.org/abs/1803.10417) <br>\n",
    " \n",
    "** bkl ** <br>\n",
    " La \"queratosis benigna\" es una clase genérica que incluye queratosis seborreica (\"verruga senil\"), lentigo solar, que puede considerarse una variante plana de la queratosis seborreica y liquenoplus como queratosis (LPLK), que corresponde a una seborréica Queratosis o un lentigo solar con inflamación y regresión [22]. Los tres subgrupos pueden tener un aspecto dermatoscópico diferente, pero los agrupamos porque son biológicamente similares y, a menudo, se informan bajo el mismo término genérico histopatológicamente. Desde un punto de vista dermatoscópico, las queratosis de tipo liquen plano son especialmente desafiantes porque pueden mostrar características morfológicas que simulan un melanoma [23] y con frecuencia se realizan biopsias o se extirpan por razones diagnósticas.\n",
    "*[1099 imágenes]*\n",
    "***\n",
    "**bcc** <br>\n",
    "El carcinoma de células basales es una variante común del cáncer de piel epitelial que rara vez hace metástasis pero crece destructivamente si no se trata. Aparece en diferentes variantes morfológicas (plana, nodular, pigmentada, quística, etc.), todas incluidas en este conjunto. <br>\n",
    "*[514 imágenes] *\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3f6843b78793e1c047ca6909a7449dc9bfc3f1c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "** akiec ** <br>\n",
    "Las queratosis actínicas (queratosis solares) y el carcinoma intraepitelial (enfermedad de Bowen) son variantes no invasivas comunes del carcinoma de células escamosas que pueden tratarse localmente sin cirugía. Algunos autores los consideran como precursores de carcinomas de células escamosas y no como carcinomas reales. Sin embargo, hay acuerdo en que estas lesiones pueden progresar a un carcinoma de células escamosas invasivo, que generalmente no está pigmentado. Ambas neoplasias comúnmente muestran escamas en la superficie y comúnmente están desprovistas de pigmento. Las queratosis actínicas son más comunes en la cara y la enfermedad de Bowen es más común en otros sitios del cuerpo. Debido a que ambos tipos están inducidos por la luz ultravioleta, la piel circundante generalmente se caracteriza por daños severos causados ​​por el sol, excepto en los casos de enfermedad de Bowen que son causados por la infección del virus del papiloma humano y no por los rayos UV. Existen variantes pigmentadas para la enfermedad de Bowen [19] y para las queratosis actínicas [20]. Ambos están incluidos en este conjunto. <br>\n",
    "*[327 imágenes]*\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** vasc ** <br>\n",
    "Las lesiones vasculares de la piel en el conjunto de datos van desde los angiomas de cereza hasta los angioqueratomas [25] y los granulomas piógenos [26]. La hemorragia también se incluye en esta categoría. <br>\n",
    "*[142 imágenes]*\n",
    "\n",
    "** df ** <br>\n",
    "El dermatofibroma es una lesión benigna de la piel considerada como una proliferación benigna o una reacción inflamatoria a un trauma mínimo. Es marrón y con frecuencia muestra una zona central de fibrosis dermatoscópica [24]. <br>\n",
    "*[115 imágenes]*\n",
    "\n",
    "\n",
    "<br> * [Imágenes totales = 10015] *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [TensorFlow](https://www.tensorflow.org/)\n",
    "\n",
    "TensorFlow es una plataforma de código abierto  para el aprendizaje automático. Cuenta con un ecosistema integral y flexible de herramientas, bibliotecas y recursos de la comunidad que permite a los investigadores impulsar el estado de la técnica en ML y los desarrolladores pueden crear y desplegar fácilmente aplicaciones potenciadas por ML.\n",
    "\n",
    "### [Keras](https://keras.io/)\n",
    "\n",
    "Keras es una interfaz de programación de aplicaciones (API) de redes neuronales de alto nivel, escrita en Python y capaz de ejecutarse sobre TensorFlow, CNTK o Theano. Fue desarrollado con un enfoque en permitir la experimentación rápida. Poder pasar de la idea al resultado con el menor retraso posible es clave para hacer una buena investigación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Aprendizaje Profundo \n",
    "\n",
    "La mayoría de los modelos modernos de aprendizaje profundo se basan en una red neuronal artificial,\n",
    "\n",
    "En el aprendizaje profundo, cada nivel aprende a transformar sus datos de entrada en una representación un poco más abstracta y compuesta. En una aplicación de reconocimiento de imágenes, la entrada sin formato puede ser una matriz de píxeles; la primera capa de representación puede abstraer los píxeles y codificar los bordes; la segunda capa puede componer y codificar arreglos de bordes; la tercera capa puede codificar una nariz y ojos; y la cuarta capa puede reconocer que la imagen contiene una cara. Es importante destacar que un proceso de aprendizaje profundo puede aprender qué características ubicar de manera óptima en qué nivel por sí solo. (Por supuesto, esto no elimina completamente la necesidad del ajuste manual; por ejemplo, un número variable de capas y tamaños de capas puede proporcionar diferentes grados de abstracción). \n",
    "\n",
    "El \"profundo\" en \"aprendizaje profundo\" se refiere al número de capas a través de las cuales se transforman los datos. Más precisamente, los sistemas de aprendizaje profundo tienen una profundidad de ruta de asignación de crédito (CAP) sustancial. El CAP es la cadena de transformaciones de entrada a salida. Los CAP describen conexiones potencialmente causales entre entrada y salida. Para una red neuronal avanzada, la profundidad de los CAP es la de la red y es el número de capas ocultas más una (ya que la capa de salida también está parametrizada). Para redes neuronales recurrentes, en las que una señal puede propagarse a través de una capa más de una vez, la profundidad de la PAC es potencialmente ilimitada. [2] No se acordó universalmente que el umbral de profundidad divide el aprendizaje superficial del aprendizaje profundo, pero la mayoría de los investigadores están de acuerdo en que el aprendizaje profundo implica una profundidad de la PAC. cita requerida] Más allá de que más capas no se agregan a la función de aproximación de la función de la red. Los modelos profundos (CAP> 2) son capaces de extraer mejores características que los modelos poco profundos y, por lo tanto, las capas adicionales ayudan en las características de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Aprendizaje Profundo y Redes Neuronales](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "#### Introducción.\n",
    "\n",
    "El sistema visual humano es una de las maravillas del mundo. Considere la siguiente secuencia de dígitos escritos a mano: \n",
    "<center> \n",
    "<a href=\"https://fontmeme.com/handwriting-fonts/\">\n",
    "<img src=\"https://fontmeme.com/permalink/190307/05e4494c338d7e33420364ec6abe0b42.png\" alt=\"handwriting-fonts\" border=\"0\"></a> \n",
    "</center>\n",
    "\n",
    "La mayoría de las personas reconocen sin esfuerzo esos dígitos como 570139. Esta facilidad engañosa. En cada hemisferio de nuestro cerebro, los humanos tienen una corteza visual primaria, también conocida como V1, que contiene 140 millones de neuronas, con decenas de miles de millones de conexiones entre ellas. Y, sin embargo, la visión humana implica no solo V1, sino toda una serie corticales visuales (V2, V3, V4 y V5) que realizan un procesamiento de imágenes cada vez más complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Aprendizaje Profundo y Redes Neuronales](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "La dificultad del reconocimiento de patrones visuales se hace evidente si intenta escribir un programa de computadora para reconocer dígitos como los anteriores. Lo que parece fácil cuando lo hacemos nosotros mismos de repente se vuelve extremadamente difícil. Las intuiciones simples sobre cómo reconocemos las formas, no son tan simples de expresar algorítmicamente. Cuando tratas de hacer precisas tales reglas, rápidamente te pierdes en una gran cantidad de excepciones y advertencias y casos especiales. Resulta desolador\n",
    "\n",
    "Las redes neuronales abordan el problema de una manera diferente. La idea es tomar un gran número de dígitos escritos a mano, conocidos como ejemplos de capacitación,\n",
    "\n",
    "<center> \n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png\"></a>  \n",
    "</center>\n",
    "\n",
    "y luego desarrollar un sistema que pueda aprender de esos ejemplos de entrenamiento. En otras palabras, la red neuronal utiliza los ejemplos para inferir automáticamente reglas para reconocer dígitos escritos a mano. Además, al aumentar el número de ejemplos de capacitación, la red puede aprender más sobre la escritura a mano, y así mejorar su precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *La mayoría de los modelos modernos de aprendizaje profundo se basan en una red neuronal artificial*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Aprendizaje Profundo y Redes Neuronales](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "#### Neuronas Artificiales: El Perceptrón\n",
    "\n",
    "Las redes de neuronales estan hechas de neuronas y existen varios tipos. Una de ellas es el perceptrón:\n",
    "\n",
    "\n",
    "<center> \n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz0.png\"></a>  \n",
    "</center>\n",
    "\n",
    "*Un perceptrón toma varias entradas binarias (tres en este caso), $x_{1}, x_{2}, x_{2}$ y produce una salida binaria única*. Una manera de pensar sobre el perceptrón es que es un dispositivo que toma decisiones al sopesar la evidencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [El Perceptrón](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "No es  muy realista, pero es fácil de entender. Suponga que se acerca el fin de semana, y has oído que habrá un festival de la cerveza en tu ciudad. Te gusta el queso y estás tratando de decidir si ir o no al festival. Puede tomar su decisión sopesando tres factores:\n",
    "\n",
    "¿El clima esta agradable?\n",
    "¿Tu novio o novia quiere acompañarte?\n",
    "¿Está el festival cerca del transporte público? (No tienes auto).\n",
    "\n",
    "Podemos representar estos tres factores mediante las variables binarias correspondientes $x_{1}, x_{2}, x_{2}$. Por ejemplo, tendríamos $x_{1} = 1$ si el clima es bueno, y $x_{1} = 0$ si el clima es malo. De manera similar, $x_{2} = 1$ si su novio o novia quiere ir, y $x_{2} = 0$ si no. Similarmente de nuevo para $x_{3} = 1$ y transporte público.\n",
    "\n",
    "Ahora, supongamos que adoras la cerveza, tanto que estás feliz de ir al festival, incluso si a tu novio o novia no le interesa y es difícil llegar al festival. Pero tal vez realmente detestas el mal tiempo, y no hay manera de que vayas al festival si el clima es malo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [El Perceptrón](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "Puede utilizar perceptrones para modelar este tipo de toma de decisiones. En este caso el perceptrón viene dado por \n",
    "![](../images/perceptron.png)\n",
    "\n",
    "Los números(pesos) 6,2,2 se han escogido de manera que reflejen el hecho que el clima es muy importante para usted, mucho más que si su novio o novia se unen a usted o la cercanía del transporte público. Finalmente, suponga que elige un umbral de 5 para el perceptrón. Con estas opciones, el perceptrón implementa el modelo de toma de decisiones deseado, generando 1 cuando el clima es bueno y 0 cuando el clima es malo. No importa el resultado si su novio o novia quiere ir, o si el transporte público está cerca. \n",
    "\n",
    "Al variar los pesos y el umbral, podemos obtener diferentes modelos de toma de decisiones. Por ejemplo, supongamos que, en cambio, elegimos un umbral de 3. El perceptrón decidirá que deberías ir al festival cuando el tiempo fuera bueno o cuando el festival estaba cerca del transporte público y tu novio o novia estaba dispuesto a unírsele. En otras palabras, sería un modelo diferente de toma de decisiones. Bajar el umbral significa que estás más dispuesto a ir al festival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [El Perceptrón](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "¡Obviamente, el perceptrón no es un modelo completo de toma de decisiones humanas! Pero lo que ilustra el ejemplo es cómo un perceptrón puede sopesar diferentes tipos de evidencia para tomar decisiones. Y debería parecer plausible que una compleja red de perceptrones pueda tomar decisiones bastante sutiles:\n",
    "<center> \n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz1.png\"></a>  \n",
    "</center>\n",
    "\n",
    "En esta red, la primera columna de perceptrones (lo que llamaremos la primera *capa* de perceptrones) está tomando tres decisiones muy simples, sopesando la evidencia de entrada. ¿Qué pasa con los perceptrones en la segunda capa? Cada uno de esos perceptrones está tomando una decisión sopesando los resultados del primer nivel de toma de decisiones. De esta manera, los  perceptrones en la sucesivos están tomando decisiones a un nivel más complejo y más abstracto que los perceptrones de sus capas anteriores:  una red de múltiples capas de perceptrones puede participar en la toma de decisiones sofisticada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [El Perceptrón](http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "<center> \n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz1.png\" withd></a>  \n",
    "</center>\n",
    "\n",
    "En la red de arriba, los perceptrones parecen tener múltiples salidas. De hecho, todavía son de salida única. Las flechas de salida múltiple son simplemente una forma útil de indicar que la salida de un perceptrón se está utilizando como entrada para varios otros perceptrones. Es menos difícil de manejar que dibujar una sola línea de salida que luego se divide.\n",
    "\n",
    "_Resulta ser que se pueden idear __algoritmos de aprendizaje__ que pueden ajustar automáticamente los pesos de una red de neuronas artificiales. Esta sintonización ocurre en respuesta a estímulos externos, sin la intervención directa de un programador._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### El problema con los perceptrones\n",
    " Supongamos que tenemos una red de perceptrones que nos gustaría usar para aprender a resolver un problema, por ejemplo, las entradas a la red podrían ser los datos de píxeles de una imagen escaneada y manuscrita de un dígito. Y nos gustaría que la red aprendiera los pesos  para que la salida de la red clasifique correctamente el dígito. Supongamos que hacemos un pequeño cambio en algo de peso en la red. Lo que nos gustaría es que este pequeño cambio en el peso cause solo un pequeño cambio correspondiente en la salida de la red:\n",
    "<center> \n",
    "<a>\n",
    "<img src=\"../images/neurona-cambios-pequenos.png\" width='70%'></a>  \n",
    "</center>\n",
    "Así la red 'aprendará' poco a poco. Desafortunadamente, esto no pasa con los percetrones: Cambios pequeños en los pesos no garantizan cambios pequeños en la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurona Sigmoide\n",
    "Podemos superar este problema introduciendo un nuevo tipo de neurona artificial llamada neurona sigmoide. Las neuronas sigmoides son similares a los perceptrones, pero se modifican de modo que pequeños cambios en sus pesos y sesgos causan solo un pequeño cambio en su producción. Ese es el hecho crucial que permitirá que una red de neuronas sigmoides aprendan. Se presentan las neuronas sigmoideas de la misma manera que describimos los perceptrones:\n",
    "<center> \n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz0.png\"></a>  \n",
    "</center>\n",
    "\n",
    "Al igual que un perceptrón, la neurona sigmoide tiene entradas, x1, x2, ... Pero en lugar de ser solo 0 o 1, estas entradas también pueden tomar cualquier valor entre 0 y 1. Entonces, por ejemplo, 0.638 ... es una entrada válida para una neurona sigmoide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurona Sigmoide\n",
    " Además, al igual que un perceptrón, la neurona sigmoide tiene pesos para cada entrada, $w_1$, $w_2$. Pero la salida no es 0 o 1. Sino la función:\n",
    "<center> \n",
    "<a >\n",
    "<img src=\"../images/sigmoide-function.png\" width=40%></a>  \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para ver por qué esta función se parece a un percetrón la graficamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(x):  \n",
    "    return  np.array([1 / (1 + np.exp(-y)) for y in x])\n",
    "\n",
    "                     \n",
    "t = np.linspace(-4,4,400)\n",
    "a = 4*sigmoid(t) - 2\n",
    "\n",
    "plt.title('Función Sigmoide')\n",
    "plt.plot(t,a)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si $\\sigma$ hubiera sido de hecho una función escalonada, entonces la neurona sigmoidea sería un perceptrón, ya que la salida sería 1 o 0 dependiendo de si $\\mathrm{peso}$ es positivo o negativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = 4*np.heaviside(t,.5) - 2\n",
    "plt.title('Función Paso')\n",
    "plt\n",
    "plt.plot(t,b,'m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Al utilizar la función $\\sigma$ real, obtenemos, como ya se indicó anteriormente, un perceptrón suavizado. De hecho, es la suavidad de la función $\\sigma$ lo que es el hecho crucial, no su forma detallada. La suavidad de $\\sigma$ significa que pequeños cambios Δ en los pesos y producirán un pequeño cambio en la salida de la neurona:\n",
    "\n",
    "$\\Delta \\mathrm{salida} = \\frac{d(\\mathrm{salida})}{d (\\mathrm{peso})}\\Delta (\\mathrm{peso})$\n",
    "\n",
    "Entonces, mientras que las neuronas sigmoideas tienen una gran parte del mismo comportamiento cualitativo que los perceptrones, hacen mucho más fácil descubrir cómo el cambio de los pesos cambiará la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que realmente importa es la forma de $\\sigma$, y no su forma exacta. De hecho, se pueden utilizar neuronas donde la salida con otras funciones activación f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):  \n",
    "    return  np.array([1 / (1 + np.exp(-y)) for y in x])\n",
    "\n",
    "def tanh(x):\n",
    "    return np.array([np.tanh(y) for y in x])\n",
    "\n",
    "t = np.linspace(-4,4,400)\n",
    "\n",
    "a = 4*sigmoid(t) - 2\n",
    "b = 1*tanh(t) + 0\n",
    "\n",
    "plt.plot(t,a,t,b)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Cómo deberíamos interpretar la salida de una neurona sigmoide? Obviamente, una gran diferencia entre los perceptrones y las neuronas sigmoideas es que las neuronas sigmoideas no solo producen la salida 0 o 1. Pueden tener como salida cualquier número real entre 0 y 1, por lo que valores como 0.173 ... y 0.689 ... son salidas legítimas. Esto puede ser útil, por ejemplo, si queremos usar el valor de salida para representar la intensidad promedio de los píxeles en una entrada de imagen a una red neuronal. Pero a veces puede ser una molestia. Supongamos que queremos que la salida de la red indique \"la imagen de entrada es un 9\" o \"la imagen de entrada no es un 9\". Obviamente, sería más fácil hacer esto si la salida fuera un 0 o un 1, como en un perceptrón. Pero en la práctica podemos establecer una convención para lidiar con esto, por ejemplo, al decidir interpretar cualquier salida de al menos 0.5 como que indica un \"9\", y cualquier salida menor a 0.5 indica que \"no es un 9\". Siempre declararé explícitamente cuando usamos dicha convención, por lo que no debería causar ninguna confusión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La arquitectura de las redes neuronales.\n",
    "\n",
    "Supongamos que tenemos la red:\n",
    "\n",
    "<center> \n",
    "<a>\n",
    "<img src=\"../images/architecture-simple.png\" width='60%'></a>  \n",
    "</center>\n",
    "\n",
    "La capa más a la izquierda en esta red se llama la *capa de entrada*, y las neuronas dentro de la capa se llaman *neuronas de entrada*. La capa de la derecha o de salida contiene las neuronas de salida, o, como en este caso, una neurona de *salida única*. La capa intermedia se denomina *capa oculta*, ya que las neuronas en esta capa no son entradas ni salidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### La arquitectura de las redes neuronales.\n",
    "\n",
    "Por ejemplo, la siguiente red de cuatro capas tiene dos capas ocultas:\n",
    "\n",
    "<center> \n",
    "<a>\n",
    "<img src=\"../images/architecture-layers.png\" width='60%'></a>  \n",
    "</center>\n",
    "\n",
    "El diseño de las capas de entrada y salida en una red a menudo es sencillo. Por ejemplo, supongamos que estamos tratando de determinar si una imagen escrita a mano representa un \"9\" o no. Una forma natural de diseñar la red es codificar las intensidades de los píxeles de la imagen en las neuronas de entrada. Si la imagen es de 64 por 64 en escala de grises, tendríamos 4,096 = 64 × 64 neuronas de entrada, con las intensidades escaladas adecuadamente entre 0 y 1. La capa de salida contendrá una sola neurona, con valores de salida inferiores a 0.5 indicando que \"la imagen de entrada no es un 9\", y valores mayores a 0.5 indicando que \"la imagen de entrada es un 9\".\n",
    "\n",
    "Si bien el diseño de las capas de entrada y salida de una red neuronal es a menudo sencillo, puede haber bastante arte en el diseño de las capas ocultas.  Hasta ahora, hemos estado discutiendo sobre redes neuronales donde la salida de una capa se usa como entrada para la siguiente capa. Estas redes se denominan *redes neuronales de avance*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [El conjunto de datos MNIST](http://yann.lecun.com/exdb/mnist/) \n",
    "\n",
    "Una vez que se tiene un  diseño de la red neuronal, ¿cómo puede aprender a reconocer los dígitos? Lo primero que se necesita es un conjunto de datos para aprender, el llamado _conjunto de datos de entrenamiento_.\n",
    "\n",
    "[El conjunto de datos MNIST](http://yann.lecun.com/exdb/mnist/)  contiene decenas de miles de imágenes escaneadas de dígitos escritos a mano, junto con sus clasificaciones correctas. El nombre de MNIST proviene del hecho de que es un subconjunto modificado de dos conjuntos de datos recopilados por NIST, el Instituto Nacional de Estándares y Tecnología de los Estados Unidos.\n",
    "\n",
    "_La formación de un clasificador con el conjunto de datos MNIST se considera el **hola mundo** del reconocimiento de imágenes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestran algunas imágenes típicas de MNIST:\n",
    "    \n",
    "<center> \n",
    "<a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/digits_separate.png\" width=50%></a>  \n",
    "</center>\n",
    "\n",
    "Los datos del MNIST vienen en dos partes. La primera parte contiene 60,000 imágenes que se utilizarán como _datos de entrenamiento_.  La segunda parte del conjunto de datos MNIST es 10,000 imágenes que se utilizarán como _datos de prueba_. Nuevamente, estas son imágenes de 28 por 28 en escala de grises. Usaremos los datos de prueba para evaluar qué tan bien nuestra red neuronal ha aprendido a reconocer dígitos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usa la notación $x$ para indicar una entrada de entrenamiento. Será conveniente considerar cada entrada de entrenamiento $x$  como un vector de 28 × 28 = 784 dimensiones. Cada entrada en el vector representa el valor de gris para un solo píxel en la imagen. La salida deseada correspondiente es $y = y (x)$, donde $y$ es un vector de 10 dimensiones. Por ejemplo, si una imagen de entrenamiento en particular, $x$, representa un 6, entonces $y (x) = (0,0,0,0,0,0,1,0,0,0)^{\\intercal}$ es la salida deseada de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Costo\n",
    "Necesitamos un algoritmo que sirva para perfeccionar los pesos de la red, de tal forma que nuestras predicciones sean mejores. Para ello, usamos la [función](https://www.codecogs.com/latex/eqneditor.php)\n",
    "<center> \n",
    "<a>\n",
    "<img src=\"../images/cost-function-only-weights.png\" width=30%></a>  \n",
    "</center>\n",
    "Aquí, $w$ denota la colección de todos los pesos en la red, $n$ es el número total de entradas de entrenamiento, $a$ es el vector de salidas de la red cuando $x$ es una entrada, y la suma es sobre todas las entradas de entrenamiento, $X$. Por supuesto, la salida a depende de $x$ y  $w$. La notación $\\vert\\vert v\\vert\\vert$ solo denota la función de longitud usual para un vector $v$. Se le llama  $C$ la función de costo cuadrático; a veces también se conoce como _error cuadrático medio (MSE)_ , _función de perdida_ o _función de objetiva_.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***EL objetivo al entrenar una red neuronal es encontrar pesos que minimicen la función de costo $C$***\n",
    "\n",
    "Aprovechando que la la función tiene derivadas, se ataca el problema usando _cálculo_ para tratar de encontrar el mínimo analíticamente. Se  calcular el gradiente de $C$ (o sea sus derivadas) y luego se usa para encontrar lugares donde $C$ es un extremo. Hacerlo directamente se convertie en una pesadilla cuando se tienen muchas más variables: las redes neuronales más grandes tienen funciones de costo que dependen de miles de millones de pesos de una manera extremadamente complicada.\n",
    "\n",
    "Para hacer esto práctico, es comun usar el  _descenso de gradiente estocástico_ y el _algoritmo de retropragación_ para acelerar el aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Descenso de gradiente estocástico](http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent)\n",
    " La idea es estimar el gradiente de $C$ para una pequeña muestra de insumos de entrenamiento elegidos al azar. Al promediar sobre esta pequeña muestra, resulta que podemos obtener rápidamente una buena estimación del gradiente verdadero, y esto ayuda a acelerar el descenso del gradiente y, por lo tanto, el aprendizaje.\n",
    " \n",
    " \n",
    "### [Algoritmo de retroprapagación](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "Se calculan los errores comenzando desde la capa final. Puede parecer extraño que estemos retrocediendo por la red. Pero se aprovecha que el retroceso es una consecuencia del hecho de que el costo es una función de las salidas de la red. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MNIST con Keras](https://nextjournal.com/gkoehler/digit-recognition-with-keras)\n",
    "\n",
    "Recapitulando: Si el costo $C$ es cero para cualquier predicción (salida de la red), entonces estas son _predicciones exactas_. Obviamente esto no es posible.  **Se busca entrenar la red mediante algoritmos que hacen que  $C$ disminuya.**\n",
    "\n",
    "Existe una amplia gama de librerias computacionales para semejante empresa. Aqui se usan las librerias **Tensorflow** y **keras** de **python**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TensorFlow](https://www.tensorflow.org/)\n",
    "\n",
    "TensorFlow es una plataforma de código abierto  para el aprendizaje automático. Cuenta con un ecosistema integral y flexible de herramientas, bibliotecas y recursos de la comunidad que permite a los investigadores impulsar el estado de la técnica en ML y los desarrolladores pueden crear y desplegar fácilmente aplicaciones potenciadas por ML.\n",
    "\n",
    "### [Keras](https://keras.io/)\n",
    "\n",
    "Keras es una interfaz de programación de aplicaciones (API) de redes neuronales de alto nivel, escrita en Python y capaz de ejecutarse sobre TensorFlow, CNTK o Theano. Fue desarrollado con un enfoque en permitir la experimentación rápida. Poder pasar de la idea al resultado con el menor retraso posible es clave para hacer una buena investigación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for array-handling and plotting\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's keep our keras backend tensorflow quiet\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "# for testing on CPU\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrique/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keras imports for the dataset and building our neural network\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargaremos el conjunto de datos utilizando esta función práctica que divide los datos del MNIST en conjuntos de entrenamiento y de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a inspeccionar algunos ejemplos. El conjunto de datos MNIST contiene solo imágenes en escala de grises. Para conjuntos de datos de imagen más avanzados, tendremos los tres canales de color (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad69dae710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(9):\n",
    "  plt.subplot(3,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Digit: {}\".format(y_train[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar la red neuronal para clasificar imágenes, primero se ponen los píxeles en un gran vector de entrada: _la capa de entrada_. Así que su longitud debe ser $28\\times 28 = 784$. Se grafica la distribución de nuestros valores de píxeles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAGntJREFUeJzt3XuYXFWd7vHvS0JAICGEBCQhIQQyIPAwEcNlFAVBBhNhQhBGOCIRAqgDggdFuXmOjMCAIGgOHIThIkTlMqKTgHpACZHxIGHC/SZDgJCEBAgkgSQIGPKbP/bqsqpS3V3dXZ3qXv1+nqefXnuvVWuvVbv612uvvWtvRQRmZpavDZrdADMz614O9GZmmXOgNzPLnAO9mVnmHOjNzDLnQG9mljkHeusTJF0u6Qd1lPuRpG+vjzaZrS8O9NarSZov6c+SVkpaIel+SV+WtEFZmY8DHwHOaK++iPhyRHw3vW5/SYsa2Nb9Ja2VtKrsZ0qj6jdrTf9mN8CsAQ6NiN9J2hzYD/ghsDdwXMrfHjgqIv7SrAaWWRwR2za7Eda3eERv2YiINyNiJvA5YIqk3VLWAcA/tZST9E1JSyQtlnSCpJC0Y8r7saTzJW0K/AYYXjb6Hi5pI0k/SK9dnNIbre++mnWEA71lJyIeBBYBH6/Ok/Rp4HTgU8COFEcAtepYDUygGIFvln4WA+cA+wDjgL8F9gLOLat/haR922jeVpJelfRiOm+waac6adYBDvSWq8XAkBrr/xG4ISKeioi3gfM6WO/ngX+OiNciYml6/RdaMiNicET8oZXX/oniH8Q2FEcZHwEu6+D2zTrMgd5yNQJYVmP9cGBh2fLCGmXaMhx4qWz5pbSuXRHxSkQ8HRFrI+JF4JvAER3cvlmHOdBbdiTtSRHoa42slwDlJ0NHtlFVrVu7Lga2K1seldZ1RgDq5GvN6uZAb9mQNEjSIcAtwE8i4okaxW4DjpP0IUmbAP+rjSpfBbZMV/O0uBk4V9IwSUPT639SZ/v2lzRKhZHARcCMel5r1hUO9JaDOyStpJiGOYdi3vu4WgUj4jfANOBeYB7wx5T1bo2yf6II7C+kk6zDgfOBucDjwBPAw2kdAOnqnHVOAid7pO2tBu4HngRO7VBPzTpBfvCI9WWSPkQRcDeKiDXNbo9Zd/CI3vocSZMlDZC0BXAxcIeDvOXMgd76oi8BS4HngfeBrzS3OWbdy1M3ZmaZ84jezCxzvqmZdRtJPlzsOV6PiGHNboQ1h0f0Zn3DS+0XsVw50JuZZc6B3swscw70ZmaZc6A3M8ucA72ZWeYc6M3MMudAb2aWOQd6M7PMOdCbmWXOgd7MLHMO9GZmmXOgNzPLnAO9mVnmHOjNzDLnQG9mljkHejOzzPkJU9bn9evXr2J58803r/u1p5xySim9ySabVOTttNNOpfTJJ59ckXfppZeW0kcffXRF3jvvvFNKX3TRRRV55513Xt1tM2vhEb2ZWeYc6M3MMuepG8vGqFGjKpYHDBhQSn/0ox+tyNt3331L6cGDB1fkffazn21IexYtWlRKT5s2rSJv8uTJpfTKlSsr8h577LFS+ve//31D2mJ9m0f0ZmaZc6A3M8ucA72ZWeYUEc1ug2VKUrd/uMaNG1dKz5o1qyKvI5dJNsLatWsrlo8//vhSetWqVa2+bsmSJRXLy5cvL6WfffbZBrWOhyJifKMqs97FI3ozs8w50JuZZc6XV1qvtmDBglL6jTfeqMhrxNTNnDlzKpZXrFhRsfzJT36ylH7vvfcq8qZPn97l7Zs1gkf0ZmaZc6A3M8ucA72ZWeY8R2+92rJly0rpM844oyLvkEMOKaUfeeSRirzqWxKUe/TRR0vpgw46qCJv9erVFcu77rprKX3aaafV0WKz9c8jejOzzDnQm5llzt+MtW6zPr4Z25ZBgwaV0tV3iLz66qtL6alTp1bkHXPMMaX0zTff3E2tW+/8zdg+zCN6M7PMOdCbmWXOgd7MLHO+vNKy9dZbb7Wa9+abb7aad+KJJ5bSt956a0Ve9R0qzXoDj+jNzDLnQG9mljlfXmndptmXV7Zl0003LaXvuOOOirz99tuvlJ4wYUJF3t133929Des+vryyD/OI3swscw70ZmaZc6A3M8uc5+it2/TkOfpyO+ywQ8Xyww8/XEpXP1Hq3nvvrVieO3duKX3llVdW5PWwvy3P0fdhHtGbmWXOgd7MLHOeurFu01umbqpNnjy5lL7hhhsq8gYOHNjq684+++yK5ZtuuqmUXrJkSYNa12meuunDPKI3M8ucA72ZWeYc6M3MMuc5eus2vXWOvtxuu+1WsXzZZZdVLB944IGtvrb8KVYXXHBBRd7LL7/cgNZ1iOfo+zCP6M3MMudAb2aWOQd6M7PMeY7euk0Oc/TVBg8eXLF86KGHltLV19xLKqVnzZpVkXfQQQd1Q+va5Dn6PswjejOzzDnQm5llzlM31m1ynLppy7vvvlux3L9//1J6zZo1FXkHH3xwKT179uxubVfiqZs+zCN6M7PMOdCbmWXOgd7MLHP92y9i1nftvvvuFctHHHFExfKee+5ZSpfPyVd7+umnK5bvu+++BrTOrD4e0ZuZZc6B3swsc566sT5vp512qlg+5ZRTSunDDz+8Iu+DH/xg3fW+//77pXT1E6bWrl3bkSaadYlH9GZmmXOgNzPLnAO9mVnmPEdvfUL13PrRRx9dSpfPyQOMHj26U9uYO3duxXL5U6VmzpzZqTrNGsEjejOzzDnQm5llzlM3lo2tt966YnmXXXYppa+44oqKvJ133rlT25gzZ07F8iWXXFJKz5gxoyLPl1BaT+ERvZlZ5hzozcwy50BvZpY5z9FbrzJkyJCK5auvvrqUHjduXEXemDFjOrWN+++/v5T+/ve/X5F31113VSz/+c9/7tQ2zNYnj+jNzDLnQG9mljlP3ViPs/fee1csn3HGGaX0XnvtVZE3YsSITm3j7bffLqWnTZtWkXfhhReW0qtXr+5U/WY9iUf0ZmaZc6A3M8ucA72ZWeY8R289zuTJk9tcbk31A7jvvPPOUnrNmjUVeeWXTa5YsaKjTTTrVTyiNzPLnAO9mVnmFBHNboNlSpI/XD3HQxExvtmNsObwiN7MLHMO9GZmmXOgNzPLnAO9mVnmHOjNzDLnQG9mljkHejOzzDnQm5llzoHezCxzDvRmZpnz3SutO70OvNTsRhgA2zW7AdY8vteNmVnmPHVjZpY5B3ozs8w50JuZZc6BvgeT9CNJ325QXaMkrZLULy3PlnRCI+qu2s4qSWOq1m0gaYak4xu4nR9LOr/OsvMlfapR2y6rNyTt2Oh6U937S1rUydfW/d40QqM+p931mTRfddM0kuYDWwNrgPeBp4GbgGsiYi1ARHy5A3WdEBG/a61MRCwANutaq9sXEbW2cQFwT0Rc393bt+4haTawD8Xn9R3gPuDkiFhS7+fUmscj+uY6NCIGUlz6dhHwLeC6Rm9EUlP/oUfEWRExrZlt6EmavT+64JT0j/xvgMHA5U1uj9XJgb4HiIg3I2Im8DlgiqTdoPIQXNJQSXdKWiFpmaT/SFMi04FRwB1p2uSbkkanaYWpkhYAs8rWlQeZHSQ9KOnNNLUyJG1rnWmD8ukPSf0knS3peUkrJT0kaWTKK01nSNpc0k2Slkp6SdK5kjZIeV+U9AdJl0paLulFSRNae48kfVjSw2l7twIbV+UfIunR9P7cL2n39t53SftIeqVlOiutmyzp8ZTeS9IfU51LJF0haUArdbXX1/8v6XJJy4Dv1Hj9B9L+Xi7paWDPqvzhkm5P9b8o6dT2+lf22hMlzUufm5mShqf1Sm16LX0GHm/57LUlIpYBtwO1PqffkvRAy+dM0lckPSVp47S8T9o/KyQ9Jmn/Ntp9vKRn0ntyl6TtutLuvsyBvgeJiAeBRcDHa2R/PeUNo5jyObt4SXwBWEBxdLBZRHyv7DX7AR8CDm5lk8cCxwPDKQ7J6x11nw4cDUwEBqU63q5R7v8AmwNjUluOBY4ry98beBYYCnwPuE6SqitJwfXfgenAEODfgM+W5e8BXA98CdgSuBqYKWmjtjoREQ8Aq4EDylb/D+BnKf0+8D9T+/4OOBD4p1aqq6evLwBbUUxlVfvfwA7p52BgSln/NgDuAB4DRqR2fE1Sa/u1RNIBwL8A/whsQ/EFtltS9t8Dn+CvI/TPAW/UUedQivf/kRrZlwDvAedKGgtcCBwTEe9IGgH8CjifYj9+A7hd0rAa2ziM4jN+OMVn/j+Am7vS7j4tIvzThB9gPvCpGusfAM5J6R8D56f0PwMzgB3bqwsYDQQwpsa6/ml5NnBRWf4uFH+g/YD9gUWtbYMiOE9qpV8B7JjqeRfYpSzvS8DslP4iMK8sb5P02g/WqPMTwGLSF/zSuvvL3purgO9WveZZYL+23uuUdz5wfUoPpAj827VS9mvALzvZ1wXtfB5eAD5dtnxSyz6g+CexoKr8WcANrdRV/rm5DvheWd5mwF/S5+EA4L8o5t43aKd9syn+ma8AXgZ+Cgyr3l7ZZ20Z8AxwVtn6bwHTq+q9C5hSto0TUvo3wNSychuk7W/XkXb7p/jxiL7nGUHxR1LtEmAecLekFySdWUddCzuQ/xKwIcXotT0jgefbKTMUGEDlLRBeouhfi1daEhHRckRQ62TucODlSH/xZXW12A74epoOWCFpRWrj8HbaCMXo/fA0+j8ceDgiXgKQ9DcqpstekfQWxei01vtTT1/b2xfDWXd/tNgOGF7Vv7MpjuzaM7y8rohYRTH6HRERs4ArgCuBVyVdI2lQG3WdGhGDI2JERHw+IpbWKhQR84F7KQL+lVX9OLKqH/tSHGlU2w74YVm5ZYA62e4+z4G+B5G0J0Vw+EN1XkSsjIivR8QY4FDgdEkHtmS3UmV797cYWZYeRTHSe51iVLtJWbv6URw+t1hIMcXQltdTfeX3WBlFMRrsqCXAiKppnVFV7bkgBaGWn00i4mbaERFPUwTCCVRO20BxpPAnYGxEDKIIrutMLVFfX9vbF0tYd3+0WAi8WNW/gRExsZ06oTgSKrVL0qYU01svA0TEtIj4CLArxVTIGXXU2SZJEymmuu6hGKCU92N6VT82jYiLalSzEPhSVdkPRMT93dXunDnQ9wCSBkk6hGLu9CcR8USNModI2jEFu7co5o/fT9mvUswNd9QxknaRtAnF1NDPI+J9isPijSV9RtKGwLlA+Xz3tcB3JY1NJ8Z2l7RlecWpntuACyQNTCfSTgd+0ol2/pHiHMKpkvpLOhzYqyz/X4EvS9o7tWfT1PaBddb/M+BUiimifytbP5DivV4laWfgK7Ve3KC+3gacJWkLSdsCXy3LexB4K53o/ICKk+G7pYFBPX07TtK4dNRyITAnIuZL2jO9ZxtS/HN/h79+pjolzd9fB5xAcZ7h0BT4oXg/DpV0cOrDxipO/G9bo6ofUbwfu6Z6N5d0ZEo3vN25c6BvrjskraQYvZwDXEblCbxyY4HfAasoAt//jYjZKe9fKE5+rZD0jQ5sfzrF/OorFFexnArFVUAUJx2vpRj5raY4EdziMorAdDdFILwO+ECN+r+aXvsCxVHKzyhOmnZIRLxHMa3yRWA5xcm3X5TlzwVOpDicX04xxfXFDmziZorzErMi4vWy9d+gGOWvpPhncmsbdXS1r+dRHFm8SPG+Tm/JSP9IDgXGpfzXKfbN5u1VGhH3AN+muEpmCcWR2FEpexBFv5anbb8BXNqBNtdyDTAjIn4dEW8AU4FrJW0ZEQuBSRRHRkspPvdnUCMORcQvgYuBW9K02ZMUR13d1e6s+e6VZmaZ84jezCxzDvRmZplzoDczy5wDvZlZ5nrEzZWGDh0ao0ePbnYzzMx6lYceeuj1iFjnFhLVekSgHz16NHPnzm12M8zMehVJL7VfylM3ZmbZc6A3M8ucA72ZWeZ6xBx9V4w+81dN2/b8iz7TtG2bmdXLI3ozs8zVFeglDZb0c0l/So/2+jtJQyT9VtJz6fcWqawkTVPx6LLH09N/zMysSeod0f8Q+H8RsTPwtxRPjjkTuCcixlLcd7rlQRgTKO60OJbiKTlXNbTFZmbWIe0G+vTklk9Q3IqWiHgvIlZQ3G70xlTsRuCwlJ4E3BSFB4DBkmo9QcbMzNaDekb0YyjuHX2DpEckXZueUrN1RCwBSL+3SuVHUPlItEVUPlINAEknSZorae7SpTWfSGZmZg1QT6DvD+wBXBURH6Z4uEJbzyut9ai1dW56HxHXRMT4iBg/bFi73+A1M7NOqifQL6J4Gv2ctPxzisD/asuUTPr9Wln58mdfbkvx3EozM2uCdgN9RLwCLJS0U1p1IPA0MJPimZCk3zNSeiZwbLr6Zh/gzZYpHjMzW//q/cLUV4GfShpA8UzM4yj+SdwmaSqwADgylf01MJHiuZ1v0/ozUM3MbD2oK9BHxKPA+BpZB9YoG8DJXWyXmZk1iL8Za2aWOQd6M7PMOdCbmWXOgd7MLHMO9GZmmXOgNzPLnAO9mVnmHOjNzDLnQG9mljkHejOzzDnQm5llzoHezCxzDvRmZplzoDczy5wDvZlZ5hzozcwy50BvZpY5B3ozs8w50JuZZc6B3swscw70ZmaZqzvQS+on6RFJd6bl7SXNkfScpFslDUjrN0rL81L+6O5pupmZ1aMjI/rTgGfKli8GLo+IscByYGpaPxVYHhE7ApencmZm1iR1BXpJ2wKfAa5NywIOAH6eitwIHJbSk9IyKf/AVN7MzJqg3hH9D4BvAmvT8pbAiohYk5YXASNSegSwECDlv5nKV5B0kqS5kuYuXbq0k803M7P2tBvoJR0CvBYRD5WvrlE06sj764qIayJifESMHzZsWF2NNTOzjutfR5mPAf8gaSKwMTCIYoQ/WFL/NGrfFlicyi8CRgKLJPUHNgeWNbzlZmZWl3ZH9BFxVkRsGxGjgaOAWRHxeeBe4IhUbAowI6VnpmVS/qyIWGdEb2Zm60dXrqP/FnC6pHkUc/DXpfXXAVum9acDZ3atiWZm1hX1TN2URMRsYHZKvwDsVaPMO8CRDWibmZk1gL8Za2aWOQd6M7PMOdCbmWXOgd7MLHMO9GZmmXOgNzPLnAO9mVnmHOjNzDLnQG9mljkHejOzzDnQm5llzoHezCxzDvRmZplzoDczy5wDvZlZ5hzozcwy50BvZpY5B3ozs8w50JuZZc6B3swsc+0GekkjJd0r6RlJT0k6La0fIum3kp5Lv7dI6yVpmqR5kh6XtEd3d8LMzFpXz4h+DfD1iPgQsA9wsqRdgDOBeyJiLHBPWgaYAIxNPycBVzW81WZmVrd2A31ELImIh1N6JfAMMAKYBNyYit0IHJbSk4CbovAAMFjSNg1vuZmZ1aVDc/SSRgMfBuYAW0fEEij+GQBbpWIjgIVlL1uU1lXXdZKkuZLmLl26tOMtNzOzutQd6CVtBtwOfC0i3mqraI11sc6KiGsiYnxEjB82bFi9zTAzsw6qK9BL2pAiyP80In6RVr/aMiWTfr+W1i8CRpa9fFtgcWOaa2ZmHVXPVTcCrgOeiYjLyrJmAlNSegowo2z9senqm32AN1umeMzMbP3rX0eZjwFfAJ6Q9GhadzZwEXCbpKnAAuDIlPdrYCIwD3gbOK6hLTYzsw5pN9BHxB+oPe8OcGCN8gGc3MV2mZlZg/ibsWZmmXOgNzPLnAO9mVnmHOjNzDLnQG9mljkHejOzzDnQm5llzoHezCxzDvRmZplzoDczy5wDvZlZ5hzozcwy50BvZpY5B3ozs8zVcz96M7OsjT7zV03b9vyLPtPt2/CI3swscw70ZmaZc6A3M8ucA72ZWeYc6M3MMudAb2aWuW4J9JI+LelZSfMkndkd2zAzs/o0/Dp6Sf2AK4GDgEXAf0qaGRFPN3pbzdasa2/Xx3W3relrfW7m9dVmjdIdX5jaC5gXES8ASLoFmARkF+ibpS8Gn77YZ7NG6Y5APwJYWLa8CNi7upCkk4CT0uIqSc92cntDgdc7+dreyP3NV1/qK7i/AOjiLtW5XT2FuiPQq8a6WGdFxDXANV3emDQ3IsZ3tZ7ewv3NV1/qK7i/61N3nIxdBIwsW94WWNwN2zEzszp0R6D/T2CspO0lDQCOAmZ2w3bMzKwODZ+6iYg1kk4B7gL6AddHxFON3k6ZLk//9DLub776Ul/B/V1vFLHO9LmZmWXE34w1M8ucA72ZWeZ6daDP/VYLkuZLekLSo5LmpnVDJP1W0nPp9xbNbmdnSbpe0muSnixbV7N/KkxL+/pxSXs0r+Wd00p/vyPp5bSPH5U0sSzvrNTfZyUd3JxWd46kkZLulfSMpKcknZbWZ7l/2+hvz9i/EdErfyhO9D4PjAEGAI8BuzS7XQ3u43xgaNW67wFnpvSZwMXNbmcX+vcJYA/gyfb6B0wEfkPxPY19gDnNbn+D+vsd4Bs1yu6SPtMbAdunz3q/ZvehA33dBtgjpQcC/5X6lOX+baO/PWL/9uYRfelWCxHxHtByq4XcTQJuTOkbgcOa2JYuiYj7gGVVq1vr3yTgpig8AAyWtM36aWljtNLf1kwCbomIdyPiRWAexWe+V4iIJRHxcEqvBJ6h+NZ8lvu3jf62Zr3u394c6GvdaqGtN7Y3CuBuSQ+lW0YAbB0RS6D4cAFbNa113aO1/uW8v09J0xXXl03FZdNfSaOBDwNz6AP7t6q/0AP2b28O9HXdaqGX+1hE7AFMAE6W9IlmN6iJct3fVwE7AOOAJcD30/os+itpM+B24GsR8VZbRWusy6G/PWL/9uZAn/2tFiJicfr9GvBLikO7V1sOadPv15rXwm7RWv+y3N8R8WpEvB8Ra4F/5a+H772+v5I2pAh6P42IX6TV2e7fWv3tKfu3Nwf6rG+1IGlTSQNb0sDfA09S9HFKKjYFmNGcFnab1vo3Ezg2XZ2xD/BmyxRAb1Y1Dz2ZYh9D0d+jJG0kaXtgLPDg+m5fZ0kScB3wTERcVpaV5f5trb89Zv82+2x1F890T6Q4u/08cE6z29Pgvo2hOCv/GPBUS/+ALYF7gOfS7yHNbmsX+ngzxeHsXyhGOFNb6x/Foe6VaV8/AYxvdvsb1N/pqT+PU/zxb1NW/pzU32eBCc1ufwf7ui/FVMTjwKPpZ2Ku+7eN/vaI/etbIJiZZa43T92YmVkdHOjNzDLnQG9mljkHejOzzDnQm5llzoHezCxzDvRmZpn7b6+pJpYV7jm2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad62ef1c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(X_train[0], cmap='gray', interpolation='none')\n",
    "plt.title(\"Dígito: {}\".format(y_train[0]))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(X_train[0].reshape(784))\n",
    "plt.title(\"Distribución del valor de los Pixeles\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se esperaba, los valores de los píxeles oscilan entre 0 y 255: la mayoría de fondo (_negro_) está cerca de 0 y los cerca de 255 (_blanco_) representan el dígito.\n",
    "\n",
    "La normalización de los datos de entrada ayuda a acelerar el entrenamiento. Además, reduce la posibilidad de atascarse en los óptimos locales, ya que estamos usando el descenso de gradiente estocástico para encontrar los pesos óptimos para la red.\n",
    "\n",
    "Cambiemos la forma de nuestras entradas a un único vector vy normalicemos los valores de píxel entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train (60000, 28, 28)\n",
      "Forma de Y_train (60000,)\n",
      "Forma de X_test (10000, 28, 28)\n",
      "Forma de Y_test (10000,)\n",
      "Forma del conjunto de entrenamiento (60000, 784)\n",
      "Forma del conjunto de prueba (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# let's print the shape before we reshape and normalize\n",
    "print(\"Forma de X_train\", X_train.shape)\n",
    "print(\"Forma de Y_train\", y_train.shape)\n",
    "print(\"Forma de X_test\", X_test.shape)\n",
    "print(\"Forma de Y_test\", y_test.shape)\n",
    "\n",
    "# building the input vector from the 28x28 pixels\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "# normalizing the data to help with the training\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# print the final input shape ready for training\n",
    "print(\"Forma del conjunto de entrenamiento\", X_train.shape)\n",
    "print(\"Forma del conjunto de prueba\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que  la _verdad_, o sea el conjunto de vectores de salida ($Y$ en la jerga del aprendizaje automático) para capacitar la red tiene sólo enteros de 0 a 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ha argumentado, las salidas (o  _categorías de salida_),  los dígitos del 0 al 9, se codifican en un vector de diez componentes. El vector es igual a cero en todas sus componenetes excepto en la posición para la categoría respectiva. Así, un $5$ estará representado por $(0,0,0,0,1,0,0,0,0,0)^{\\intercal}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma antes de una 'codificación instantánea':  (60000,)\n",
      "Forma después de una 'codificación instantánea':  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Codificación instantánea utilizando las utilidades de relacionadas de numpy y  keras.\n",
    "n_classes = 10\n",
    "print(\"Forma antes de una 'codificación instantánea': \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Forma después de una 'codificación instantánea': \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro vector de píxeles sirve como entrada. Luego, dos capas ocultas de 512 nodos, con suficiente complejidad de modelo para reconocer dígitos. Para la clasificación de múltiples clases, agregamos otra capa densamente conectada (o totalmente conectada) para las 10 clases de salida diferentes. Para esta arquitectura de red podemos usar el modelo secuencial de Keras. Podemos apilar capas utilizando el método .add ()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Construyendo la red](http://alexlenail.me/NN-SVG/index.html)\n",
    "\n",
    "Vamos a recurrir a Keras para construir una red neuronal.\n",
    "<center> \n",
    "<a>\n",
    "<img src=\"../images/neural-net-concrete-keras.png\" width='60%'></a>  \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro vector de píxeles sirve como entrada. Luego, dos capas ocultas de 512 neuronas (nodos). Para esta arquitectura de red podemos usar _el modelo secuencial de Keras_, que es el modelo de neurona que se ha discutido con anterioridad. Podemos apilar capas utilizando el método _.add()_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densidad \n",
    "Cuando cada neurona de entrada está conectada a cada neurona salida por un peso se dice que la red esta  _densamente conectada_(o _totalmente conectada_)\n",
    "\n",
    "Para la clasificación de múltiples clases, agregamos otra capa _densamente conectada_(o _totalmente conectada_) para las 10 clases de neuronas de salida diferentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))                            \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al agregar la primera capa en el Modelo Secuencial, se necesita especificar la forma de entrada para que _Keras_ pueda crear las matrices apropiadas. Para todas las capas restantes la forma se deduce automáticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para introducir no-linealidades en la red y elevarla más allá de las capacidades de un simple perceptrón, también agregamos funciones de activación a las capas ocultas. La diferenciación para el entrenamiento a través de la _propagación hacia atrás_ se está produce entre bastidores sin tener que implementar los detalles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También agregamos abandono como una forma de evitar el sobreajuste. Aquí mantenemos al azar algunos pesos de red fijos cuando normalmente los actualizaríamos para que la red no dependa demasiado de muy pocos nodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última capa consta de conexiones para nuestras 10 clases y la activación de softmax que es estándar para los objetivos de varias clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import EventCollection\n",
    "import numpy as np\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "# create random data\n",
    "xdata = np.random.random([2, 10])\n",
    "\n",
    "# split the data into two parts\n",
    "xdata1 = xdata[0, :]\n",
    "xdata2 = xdata[1, :]\n",
    "\n",
    "# sort the data so it makes clean curves\n",
    "xdata1.sort()\n",
    "xdata2.sort()\n",
    "\n",
    "# create some y data points\n",
    "ydata1 = xdata1 ** 2\n",
    "ydata2 = 1 - xdata2 ** 3\n",
    "\n",
    "# plot the data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(xdata1, ydata1, 'r', xdata2, ydata2, 'b')\n",
    "\n",
    "# create the events marking the x data points\n",
    "xevents1 = EventCollection(xdata1, color=[1, 0, 0], linelength=0.05)\n",
    "xevents2 = EventCollection(xdata2, color=[0, 0, 1], linelength=0.05)\n",
    "\n",
    "# create the events marking the y data points\n",
    "yevents1 = EventCollection(ydata1, color=[1, 0, 0], linelength=0.05,\n",
    "                           orientation='vertical')\n",
    "yevents2 = EventCollection(ydata2, color=[0, 0, 1], linelength=0.05,\n",
    "                           orientation='vertical')\n",
    "\n",
    "# add the events to the axis\n",
    "ax.add_collection(xevents1)\n",
    "ax.add_collection(xevents2)\n",
    "ax.add_collection(yevents1)\n",
    "ax.add_collection(yevents2)\n",
    "\n",
    "# set the limits\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "ax.set_title('line plot with data points')\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "371731306c3e504b191979706e826c247def88dc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import keras\n",
    "#from keras import backend as K\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "371731306c3e504b191979706e826c247def88dc"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import keras\n",
    "#from keras import backend as K\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "371731306c3e504b191979706e826c247def88dc"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "#Semilla para el generador de números aleatorios de numpy\n",
    "seed(101)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "#Semilla para el generador de números aleatorios de tensorflow\n",
    "set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import keras\n",
    "#from keras import backend as K\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5a0a200bfc57c5489eaa930255d9420a7d01c47"
   },
   "outputs": [],
   "source": [
    "os.listdir('../input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "086162161ba405b800863e7d545b5917e5205984"
   },
   "source": [
    "### Create the directory structure\n",
    "\n",
    "In these folders we will store the images that will later be fed to the Keras generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d24ef21f9f2359b8bf6b3e7a0b8ab5a43daaf566"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a new directory\n",
    "base_dir = 'base_dir'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "\n",
    "#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n",
    "\n",
    "# now we create 7 folders inside 'base_dir':\n",
    "\n",
    "# train_dir\n",
    "    # nv\n",
    "    # mel\n",
    "    # bkl\n",
    "    # bcc\n",
    "    # akiec\n",
    "    # vasc\n",
    "    # df\n",
    " \n",
    "# val_dir\n",
    "    # nv\n",
    "    # mel\n",
    "    # bkl\n",
    "    # bcc\n",
    "    # akiec\n",
    "    # vasc\n",
    "    # df\n",
    "\n",
    "# create a path to 'base_dir' to which we will join the names of the new folders\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "\n",
    "# [CREATE FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n",
    "# Inside each folder we create seperate folders for each class\n",
    "\n",
    "# create new folders inside train_dir\n",
    "nv = os.path.join(train_dir, 'nv')\n",
    "os.mkdir(nv)\n",
    "mel = os.path.join(train_dir, 'mel')\n",
    "os.mkdir(mel)\n",
    "bkl = os.path.join(train_dir, 'bkl')\n",
    "os.mkdir(bkl)\n",
    "bcc = os.path.join(train_dir, 'bcc')\n",
    "os.mkdir(bcc)\n",
    "akiec = os.path.join(train_dir, 'akiec')\n",
    "os.mkdir(akiec)\n",
    "vasc = os.path.join(train_dir, 'vasc')\n",
    "os.mkdir(vasc)\n",
    "df = os.path.join(train_dir, 'df')\n",
    "os.mkdir(df)\n",
    "\n",
    "\n",
    "\n",
    "# create new folders inside val_dir\n",
    "nv = os.path.join(val_dir, 'nv')\n",
    "os.mkdir(nv)\n",
    "mel = os.path.join(val_dir, 'mel')\n",
    "os.mkdir(mel)\n",
    "bkl = os.path.join(val_dir, 'bkl')\n",
    "os.mkdir(bkl)\n",
    "bcc = os.path.join(val_dir, 'bcc')\n",
    "os.mkdir(bcc)\n",
    "akiec = os.path.join(val_dir, 'akiec')\n",
    "os.mkdir(akiec)\n",
    "vasc = os.path.join(val_dir, 'vasc')\n",
    "os.mkdir(vasc)\n",
    "df = os.path.join(val_dir, 'df')\n",
    "os.mkdir(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ae8d37fdee293aaffa71a79019dd7277f8288fc"
   },
   "source": [
    "### Create Train and Val Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "268503398ef61904e05a2c0b0667d589f08a19a8"
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('../input/HAM10000_metadata.csv')\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c46ea5967e095d31dcf144b6f57f0343878fa432"
   },
   "source": [
    "### Create a stratified val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53e4b7b152ed831a7d7516156ac300c0e6985ffc"
   },
   "outputs": [],
   "source": [
    "# this will tell us how many images are associated with each lesion_id\n",
    "df = df_data.groupby('lesion_id').count()\n",
    "\n",
    "# now we filter out lesion_id's that have only one image associated with it\n",
    "df = df[df['image_id'] == 1]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24720fe3ea9f2f4b571abd09ecfbb931d6429852"
   },
   "outputs": [],
   "source": [
    "# here we identify lesion_id's that have duplicate images and those that have only\n",
    "# one image.\n",
    "\n",
    "def identify_duplicates(x):\n",
    "    \n",
    "    unique_list = list(df['lesion_id'])\n",
    "    \n",
    "    if x in unique_list:\n",
    "        return 'no_duplicates'\n",
    "    else:\n",
    "        return 'has_duplicates'\n",
    "    \n",
    "# create a new colum that is a copy of the lesion_id column\n",
    "df_data['duplicates'] = df_data['lesion_id']\n",
    "# apply the function to this new column\n",
    "df_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08b7eef3e0ac4112f63b8fb26ce19d55483cbc04"
   },
   "outputs": [],
   "source": [
    "df_data['duplicates'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "995445dfda2745165a53e61f42615104b951d4af"
   },
   "outputs": [],
   "source": [
    "# now we filter out images that don't have duplicates\n",
    "df = df_data[df_data['duplicates'] == 'no_duplicates']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "39fde25b59a9452cf700c5b2ff82cc7cc45c4a33"
   },
   "outputs": [],
   "source": [
    "# now we create a val set using df because we are sure that none of these images\n",
    "# have augmented duplicates in the train set\n",
    "y = df['dx']\n",
    "\n",
    "_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n",
    "\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1df37227f7ce993d054ed5b8480ee724696fc210"
   },
   "outputs": [],
   "source": [
    "df_val['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08c5e12fcef2da5f49267a6b82161b2c52c2b20a"
   },
   "source": [
    "### Create a train set that excludes images that are in the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03715a6cf5aeb6430ee144a84eb10dde216c0fb9"
   },
   "outputs": [],
   "source": [
    "# This set will be df_data excluding all rows that are in the val set\n",
    "\n",
    "# This function identifies if an image is part of the train\n",
    "# or val set.\n",
    "def identify_val_rows(x):\n",
    "    # create a list of all the lesion_id's in the val set\n",
    "    val_list = list(df_val['image_id'])\n",
    "    \n",
    "    if str(x) in val_list:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'train'\n",
    "\n",
    "# identify train and val rows\n",
    "\n",
    "# create a new colum that is a copy of the image_id column\n",
    "df_data['train_or_val'] = df_data['image_id']\n",
    "# apply the function to this new column\n",
    "df_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n",
    "   \n",
    "# filter out train rows\n",
    "df_train = df_data[df_data['train_or_val'] == 'train']\n",
    "\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
